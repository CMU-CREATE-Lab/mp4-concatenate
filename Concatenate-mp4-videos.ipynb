{
 "metadata": {
  "name": "",
  "signature": "sha256:12e3b1c2a2243d3eaef163cac9b7e94c5152e478a06ff0a314fad6e5f6db4b1e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Append to MP4 video in-place\n",
      "-------------------------------\n",
      "\n",
      "Append to an mp4 video, modifying the original video in-place to minimize I/O.  Created specifically for [Time Machine](http://timemachine.cmucreatelab.org/wiki/Main_Page).  Ignores audio, and only tested on videos created by ffmpeg by Time Machine.\n",
      "\n",
      "For the purpose of this tool, mp4 videos have 4 sections:\n",
      "- header (\"ftyp\" section)\n",
      "- movie information (\"moov\" section), which contains lots of metadata and indexes\n",
      "- free space (\"free\" section)\n",
      "- mp4-compressed video frames, concatenated into \"chunks\", which are then concatenated into the \"mdat\" section\n",
      "\n",
      "This tool concatenates videos at granularity of \"chunks\", meaning if a video is composed of multiple chunks, those chunks can be independently selected for putting into the resulting video.\n",
      "\n",
      "If you consider the append operation A += B, A will be modified in place to include some or all of B at the end.  Not all of the chunks from A or B are required to be in the final video.  But any chunks removed from A must be removed from the end of the video, so that the original frames remaining in A start at time=0 and will not need to be moved in the file.\n",
      "\n",
      "As the video A grows through successive append operations, the indexes in the \"moov\" atom will grow.  To prevent needing to relocate the potentially quite large \"mdat\" section, we use a \"free\" section which we can shrink in-place as \"mdat\" grows.  But if the \"mdat\" grows too large and exhausts the free space, A will need to be completely rewritten, with \"mdat\" moving.  This is very likely to happen the first time you append to A, since A probably won't have originally been created with a free section of significant size.  (And sometimes A will be created with the \"moov\" section after the \"mdat\", which reduces streaming efficiency -- see discussions around the \"qtfaststart\" tool).  So expect A to be rewritten on the first append.\n",
      "\n",
      "When A is rewritten to include more free space, it's useful to know if there will be more appends in the future, and if so, much free space should be included now to allow for those future appends to not require rewriting to move \"mdat\".  You can specify a number of frames, in which case the additional free space will be created to allow roughly that number of frames to be appended before needing to rewrite the video.  However, when rewriting, the tool will refuse to create a free area smaller thant he current \"mdat\" area, meaning there should be at least enough space to double the video size.  So worst case if you chronically estimate too low, the video will be rewritten log(n) times over the long haul.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Reference:  https://developer.apple.com/library/mac/documentation/QuickTime/QTFF/QTFFChap2/qtff2.html#//apple_ref/doc/uid/TP40000939-CH204-56313\n",
      "        "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import copy, json, os, pprint, re, StringIO, struct, urllib\n",
      "from collections import OrderedDict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "class AtomWriter:\n",
      "    def __init__(self, atom):\n",
      "        self.out = StringIO.StringIO()\n",
      "        self.write(atom['atomtype'])\n",
      "        self.write(atom['version'])\n",
      "        self.write(atom['flags'])\n",
      "        assert len(self.out.getvalue()) == 8\n",
      "    \n",
      "    def write16(self, val):\n",
      "        self.write(struct.pack('!H', val))\n",
      "        \n",
      "    def write32(self, val):\n",
      "        self.write(struct.pack('!I', val))\n",
      "        \n",
      "    def write(self, bytes):\n",
      "        self.out.write(bytes)\n",
      "\n",
      "    def atom(self):\n",
      "        data = self.out.getvalue()\n",
      "        return struct.pack('!I', 4 + len(data)) + data\n",
      "\n",
      "class AtomReader:\n",
      "    def __init__(self, inp):\n",
      "        self.inp = inp\n",
      "        self.position = inp.tell()\n",
      "        self.atomsize = self.read32()\n",
      "        self.atomtype = self.inp.read(4)\n",
      "        self.parsed = {\n",
      "            '_position': self.position,\n",
      "            'atomsize': self.atomsize,\n",
      "            'atomtype': self.atomtype\n",
      "        }\n",
      "        self.verbose = False\n",
      "        if self.verbose:\n",
      "            print 'Reading %s (length %d) from position %d' % (self.atomtype, self.atomsize, self.position)\n",
      "    \n",
      "    def read(self, n):\n",
      "        return self.inp.read(n)\n",
      "    \n",
      "    def read16(self):\n",
      "        return struct.unpack('!H', self.read(2))[0]\n",
      "\n",
      "    def read32(self):\n",
      "        return struct.unpack('!I', self.read(4))[0]\n",
      "    \n",
      "    def read_version_and_flags(self):\n",
      "        self.parsed['version'] = self.inp.read(1)\n",
      "        self.parsed['flags'] = self.inp.read(3)\n",
      "    \n",
      "    # Seek to end of atom and return parse\n",
      "    def skip(self):\n",
      "        self.inp.seek(self.position + self.atomsize)\n",
      "        return self.parsed\n",
      "    \n",
      "    def done(self):\n",
      "        if self.inp.tell() != self.position + self.atomsize:\n",
      "            raise Exception('While reading %s, atom length is %d but read %d bytes' % (self.atomtype, self.atomsize,\n",
      "                                                                                       self.inp.tell() - self.position))\n",
      "        return self.parsed\n",
      "    \n",
      "    def set(self, key, value):\n",
      "        self.parsed[key] = value\n",
      "        \n",
      "    def get(self, key):\n",
      "        return self.parsed[key]\n",
      "        \n",
      "class Chunk:\n",
      "    def __init__(self, video, chunkno):\n",
      "        self.video = video\n",
      "        self.chunkno = chunkno\n",
      "        chunk_offsets = video.info['moov']['trak']['mdia']['minf']['stbl']['stco']['chunk_offsets']\n",
      "        self.offset = chunk_offsets[chunkno]\n",
      "        # Compute first and last sample #s\n",
      "        self._compute_samples()\n",
      "        \n",
      "    def _compute_samples(self):\n",
      "        # Find first (includisve) and last (exclusive) sample #s\n",
      "        sample_to_chunk_map = self.video.info['moov']['trak']['mdia']['minf']['stbl']['stsc']['sample_to_chunk_map']\n",
      "        last_sample = 0\n",
      "        first_sample = 0\n",
      "        for i in range(0, self.chunkno + 1):\n",
      "            chunk_info = sample_to_chunk_map[i]\n",
      "            assert chunk_info['first_chunk'] == i + 1\n",
      "            assert chunk_info['sample_description_id'] == 1\n",
      "            first_sample = last_sample\n",
      "            last_sample += chunk_info['samples_per_chunk']\n",
      "        \n",
      "        # Find sample sizes\n",
      "        sample_sizes = self.video.info['moov']['trak']['mdia']['minf']['stbl']['stsz']['sample_sizes']\n",
      "        self.sample_sizes = sample_sizes[first_sample : last_sample]\n",
      "        self.length = sum(self.sample_sizes)\n",
      "        \n",
      "        # Find keyframes\n",
      "        key_frame_samples = self.video.info['moov']['trak']['mdia']['minf']['stbl']['stss']['key_frame_samples']\n",
      "        self.keyframes = []\n",
      "        for key_frame_sample in key_frame_samples:\n",
      "            key_frame_sample -= 1  # from 1-based to 0-based\n",
      "            if first_sample <= key_frame_sample and key_frame_sample < last_sample:\n",
      "                self.keyframes.append(key_frame_sample - first_sample)\n",
      "        \n",
      "    def __repr__(self):\n",
      "        return ('Chunk(video=%s, index=%d, offset=%d, nsamples=%d, length=%d)' % \n",
      "                (self.video.filename, self.chunkno, self.offset, \n",
      "                 len(self.sample_sizes), self.length))\n",
      "\n",
      "class NeedsRewriteException(Exception):\n",
      "    def __init__(self, why, space_needed):\n",
      "        self.value = 'Video needs rewriting because %s (space needed=%d)' % (why, space_needed)\n",
      "        self.space_needed = space_needed\n",
      "    def __str__(self):\n",
      "        return repr(self.value)\n",
      "\n",
      "class MP4:\n",
      "    def __init__(self, filename, writable=False):\n",
      "        self.writable = writable\n",
      "        self.filename = filename\n",
      "        self.fp = open(filename, 'r+' if writable else 'r')\n",
      "        self.verbose = False\n",
      "        self.info = self.parse_container()\n",
      "        print 'Read %s' % filename\n",
      "        \n",
      "    def write32(self, val):\n",
      "        return struct.pack('!I', val)\n",
      "    \n",
      "    def read32(self, bytes):\n",
      "        return struct.unpack('!I', bytes)[0]\n",
      "\n",
      "    def parse_mvhd(self, ar):\n",
      "        ar.read_version_and_flags()\n",
      "        ar.set('creation_time', ar.read32())\n",
      "        ar.set('modification_time', ar.read32())\n",
      "        ar.set('time_scale', ar.read32())\n",
      "        ar.set('duration', ar.read32()) # scale by 1 / ar.get('time_scale'))\n",
      "        ar.set('unparsed', ar.read(4 + 2 + 10 + 36 + 4*7))\n",
      "        \n",
      "    def unparse_mvhd(self, atom):\n",
      "        aw = AtomWriter(atom)\n",
      "        aw.write32(atom['creation_time'])\n",
      "        aw.write32(atom['modification_time'])\n",
      "        aw.write32(atom['time_scale'])\n",
      "        aw.write32(atom['duration'])\n",
      "        aw.write(atom['unparsed'])\n",
      "        return aw.atom()\n",
      "\n",
      "    def parse_tkhd(self, ar):\n",
      "        ar.read_version_and_flags()\n",
      "        ar.set('creation_time', ar.read32())\n",
      "        ar.set('modification_time', ar.read32())\n",
      "        ar.set('track_id', ar.read32())\n",
      "        ar.set('reserved1', ar.read(4))\n",
      "        ar.set('duration', ar.read32()) # scale by 1 / mvhd['time_scale']\n",
      "        ar.set('unparsed', ar.read(8 + 2*4 + 36))\n",
      "        ar.set('track_width', ar.read32()) # scale by 1/65536\n",
      "        ar.set('track_height', ar.read32()) # scale by 1/65536\n",
      "\n",
      "    def unparse_tkhd(self, atom):\n",
      "        aw = AtomWriter(atom)\n",
      "        aw.write32(atom['creation_time'])\n",
      "        aw.write32(atom['modification_time'])\n",
      "        aw.write32(atom['track_id'])\n",
      "        assert len(atom['reserved1']) == 4\n",
      "        aw.write(atom['reserved1'])\n",
      "        aw.write32(atom['duration'])\n",
      "        assert len(atom['unparsed']) == 8 + 2*4 + 36\n",
      "        aw.write(atom['unparsed'])\n",
      "        aw.write32(atom['track_width'])\n",
      "        aw.write32(atom['track_height'])\n",
      "        return aw.atom()\n",
      "    \n",
      "    def parse_elst(self, ar):\n",
      "        ar.read_version_and_flags()\n",
      "        count = ar.read32()\n",
      "        edits = []\n",
      "        for i in range(0, count):\n",
      "            elt = {}\n",
      "            elt['duration'] = ar.read32() # scale by 1 / mvhd['time_scale']\n",
      "            elt['start_time'] = ar.read32()\n",
      "            elt['rate'] = ar.read32() # scale by 1/65536\n",
      "            edits.append(elt)\n",
      "        ar.set('edits', edits)\n",
      "\n",
      "    def unparse_elst(self, atom):\n",
      "        aw = AtomWriter(atom)\n",
      "        aw.write32(len(atom['edits']))\n",
      "        for edit in atom['edits']:\n",
      "            aw.write32(edit['duration'])\n",
      "            aw.write32(edit['start_time'])\n",
      "            aw.write32(edit['rate'])\n",
      "        return aw.atom()\n",
      "\n",
      "    def parse_mdhd(self, ar):\n",
      "        ar.read_version_and_flags()\n",
      "        ar.set('creation_time', ar.read32())\n",
      "        ar.set('modification_time', ar.read32())\n",
      "        ar.set('time_scale', ar.read32())\n",
      "        ar.set('duration', ar.read32()) # scale by 1/time_scale\n",
      "        ar.set('language', ar.read16())\n",
      "        ar.set('quality', ar.read16())\n",
      "\n",
      "    def unparse_mdhd(self, atom):\n",
      "        aw = AtomWriter(atom)\n",
      "        aw.write32(atom['creation_time'])\n",
      "        aw.write32(atom['modification_time'])\n",
      "        aw.write32(atom['time_scale'])\n",
      "        aw.write32(atom['duration'])\n",
      "        aw.write16(atom['language'])\n",
      "        aw.write16(atom['quality'])\n",
      "        return aw.atom()\n",
      "\n",
      "    # Chunk offset table\n",
      "    def parse_stco(self, ar):\n",
      "        ar.read_version_and_flags()\n",
      "        num = ar.read32()\n",
      "        ret = []\n",
      "        for i in range(0, num):\n",
      "            ret.append(ar.read32())\n",
      "        ar.set('chunk_offsets', ret)\n",
      "\n",
      "    def unparse_stco(self, atom):\n",
      "        aw = AtomWriter(atom)\n",
      "        chunk_offsets = atom['chunk_offsets']\n",
      "        aw.write32(len(chunk_offsets))\n",
      "        for offset in chunk_offsets:\n",
      "            aw.write32(offset)\n",
      "        return aw.atom()\n",
      "    \n",
      "    # Sample size table.  For every frame, how large is it in bytes?\n",
      "    def parse_stsz(self, ar):\n",
      "        ar.read_version_and_flags()\n",
      "        sample_size = ar.read32()\n",
      "        if sample_size != 0:\n",
      "            raise Exception('sample_size of != 0 is unimplemented' % sample_size)\n",
      "        num = ar.read32()\n",
      "        sample_sizes = []\n",
      "        for i in range(0, num):\n",
      "            sample_sizes.append(ar.read32())\n",
      "        ar.set('sample_sizes', sample_sizes)\n",
      "        \n",
      "    def unparse_stsz(self, atom):\n",
      "        aw = AtomWriter(atom)\n",
      "        aw.write32(0) # fixed sample_size = 0 means samples are of variable size\n",
      "        aw.write32(len(atom['sample_sizes']))\n",
      "        for sample_size in atom['sample_sizes']:\n",
      "            aw.write32(sample_size)\n",
      "        return aw.atom()\n",
      "        \n",
      "    # Sample to chunk map\n",
      "    def parse_stsc(self, ar):\n",
      "        ar.read_version_and_flags()\n",
      "        num = ar.read32()\n",
      "        ret = []\n",
      "        for i in range(0, num):\n",
      "            entry = {}\n",
      "            entry['first_chunk'] = ar.read32()\n",
      "            entry['samples_per_chunk'] = ar.read32()\n",
      "            entry['sample_description_id'] = ar.read32()\n",
      "            ret.append(entry)\n",
      "        ar.set('sample_to_chunk_map', ret)\n",
      "\n",
      "    def unparse_stsc(self, atom):\n",
      "        aw = AtomWriter(atom)\n",
      "        aw.write32(len(atom['sample_to_chunk_map']))\n",
      "        for entry in atom['sample_to_chunk_map']:\n",
      "            aw.write32(entry['first_chunk'])\n",
      "            aw.write32(entry['samples_per_chunk'])\n",
      "            aw.write32(entry['sample_description_id'])\n",
      "        return aw.atom()\n",
      "\n",
      "    # stss: sync to sample (keyframes)\n",
      "    def parse_stss(self, ar):\n",
      "        ar.read_version_and_flags()\n",
      "        num = ar.read32()\n",
      "        key_frame_samples = []\n",
      "        for i in range(0, num):\n",
      "            key_frame_samples.append(ar.read32())\n",
      "        ar.set('key_frame_samples', key_frame_samples)\n",
      "\n",
      "    # stss: sync to sample (keyframes)\n",
      "    def unparse_stss(self, atom):\n",
      "        aw = AtomWriter(atom)\n",
      "        aw.write32(len(atom['key_frame_samples']))\n",
      "        for key_frame_sample in atom['key_frame_samples']:\n",
      "            aw.write32(key_frame_sample)\n",
      "        return aw.atom()\n",
      "\n",
      "    # Time to sample\n",
      "    def parse_stts(self, ar):\n",
      "        ar.read_version_and_flags()\n",
      "        num = ar.read32()\n",
      "        ret = []\n",
      "        for i in range(0, num):\n",
      "            entry = {}\n",
      "            entry['sample_count'] = ar.read32()\n",
      "            entry['sample_duration'] = ar.read32() # scale using mdhd.time_scale\n",
      "            ret.append(entry)\n",
      "        ar.set('time_to_sample_map', ret)\n",
      "    \n",
      "    # Time to sample\n",
      "    def unparse_stts(self, atom):\n",
      "        aw = AtomWriter(atom)\n",
      "        aw.write32(len(atom['time_to_sample_map']))\n",
      "        for entry in atom['time_to_sample_map']:\n",
      "            aw.write32(entry['sample_count'])\n",
      "            aw.write32(entry['sample_duration'])\n",
      "        return aw.atom()\n",
      "    \n",
      "    def parse_container(self, offset0=0, offset1=None, prefix=''):\n",
      "        \"Walk the atom tree in a mp4 file\"\n",
      "        if offset1 == None:\n",
      "            self.fp.seek(0, 2)\n",
      "            offset1 = self.fp.tell()\n",
      "            self.fp.seek(offset0)\n",
      "        offset= offset0\n",
      "        ret = OrderedDict()\n",
      "        while offset < offset1:\n",
      "            if self.fp.tell() != offset:\n",
      "                raise Exception('File offset is %d, but expected %d, in parse_container' % (self.fp.tell(), offset))\n",
      "            ar = AtomReader(self.fp)\n",
      "            \n",
      "            parser_name = 'parse_' + ar.atomtype\n",
      "            if parser_name in dir(self):\n",
      "                if self.verbose:\n",
      "                    print 'Found %s size %d' % (prefix + atomtype, atomsize)\n",
      "                getattr(self, parser_name)(ar)\n",
      "                val = ar.done()\n",
      "                test_unparse = False\n",
      "                if test_unparse:\n",
      "                    self.fp.seek(offset)\n",
      "                    bytes = self.fp.read(ar.atomsize)\n",
      "                    if bytes == getattr(self, 'unparse_' + ar.atomtype)(val):\n",
      "                        print 'parse and unparse match'\n",
      "                    else:\n",
      "                        raise Exception('parse and unparse do not match')\n",
      "            elif ar.atomtype in ['meta', 'moov', 'trak', 'mdia', 'minf', 'edts', 'dinf',\n",
      "                                 'stbl', 'udta']:\n",
      "                container_header = ''\n",
      "                if ar.atomtype == 'meta':\n",
      "                    container_header = ar.read(4)\n",
      "                val = self.parse_container(offset + 8 + len(container_header), offset + ar.atomsize, prefix + ar.atomtype + '.')\n",
      "                val['_container_header'] = container_header\n",
      "                for (k, v) in ar.done().iteritems():\n",
      "                    val[k] = v\n",
      "            elif ar.atomtype in ['ftyp', 'mdhd', 'hdlr', 'mdat',\n",
      "                                 'vmhd', 'dref', 'stsd', 'stts', 'stss', 'stsc',\n",
      "                                 'stsz', 'stco', 'ilst', 'free']:\n",
      "                val = ar.skip()\n",
      "            else:\n",
      "                raise Exception('Unknown atom type \"%s\"' % ar.atomtype)\n",
      "            ret[ar.atomtype] = val\n",
      "            offset += ar.atomsize\n",
      "        return ret\n",
      "\n",
      "    def write_atom(self, atom):\n",
      "        unparser_name = 'unparse_' + atom['atomtype']\n",
      "        if unparser_name in dir(self):\n",
      "            ret = getattr(self, unparser_name)(atom)\n",
      "        elif '_container_header' in atom:\n",
      "            ret = atom['atomtype']\n",
      "            ret += atom['_container_header']\n",
      "            for (child_type, child_atom) in atom.iteritems():\n",
      "                if len(child_type) == 4:\n",
      "                    ret += self.write_atom(child_atom)\n",
      "            ret = self.write32(len(ret) + 4) + ret\n",
      "        else:\n",
      "            self.fp.seek(atom['_position'])\n",
      "            ret = self.fp.read(atom['atomsize'])\n",
      "        assert self.read32(ret[0:4]) == len(ret)\n",
      "        return ret\n",
      "    \n",
      "    def write_free_atom(self, space):\n",
      "        return self.write32(space + 8) + 'free' + ('\\x00' * space)\n",
      "\n",
      "    def find_atom(self, atomtype):\n",
      "        return self._find_atom(self.info, atomtype)\n",
      "    \n",
      "    def _find_atom(self, container, atomtype):\n",
      "        for (key, val) in container.iteritems():\n",
      "            if key == atomtype:\n",
      "                return atomtype\n",
      "            if len(key) == 4:\n",
      "                ret = self._find_atom(val, atomtype)\n",
      "                if ret:\n",
      "                    return key + '.' + ret\n",
      "        return None\n",
      "    \n",
      "    def copy_with_padding(self, dest, padding):\n",
      "        moov = copy.deepcopy(self.info['moov'])\n",
      "        new_mdat_location = (self.info['ftyp']['atomsize'] +\n",
      "                             self.info['moov']['atomsize'] +\n",
      "                             8 +\n",
      "                             padding)\n",
      "        mdat_move = new_mdat_location - self.info['mdat']['_position']\n",
      "        print 'Moving mdat by %d bytes' % mdat_move\n",
      "        stco = moov['trak']['mdia']['minf']['stbl']['stco']\n",
      "        chunk_offsets = stco['chunk_offsets']\n",
      "        for i in range(0, len(chunk_offsets)):\n",
      "            chunk_offsets[i] += mdat_move\n",
      "                \n",
      "        with open(dest, 'w') as out:\n",
      "            out.write(self.write_atom(self.info['ftyp']))\n",
      "            out.write(self.write_atom(moov))\n",
      "            out.write(self.write_free_atom(padding))\n",
      "            out.write(self.write_atom(self.info['mdat']))\n",
      "            \n",
      "        print 'Wrote %s' % dest\n",
      "    \n",
      "    def chunks(self):\n",
      "        chunk_offsets = self.info['moov']['trak']['mdia']['minf']['stbl']['stco']['chunk_offsets']\n",
      "        chunks = [Chunk(self, i) for i in range(0, len(chunk_offsets))]\n",
      "        \n",
      "        # Check that chunks are contiguous in the mdat\n",
      "        pos = self.info['mdat']['_position'] + 8\n",
      "        for (i, chunk) in enumerate(chunks):\n",
      "            if chunk.offset != pos:\n",
      "                raise Exception('Expected chunk %d to start at %d, but stco says %d' % (i, pos, chunk.offset))\n",
      "            pos += chunk.length\n",
      "        assert pos == self.info['mdat']['_position'] + self.info['mdat']['atomsize']\n",
      "        return chunks\n",
      "    \n",
      "    def update_in_place_using_chunks(self, chunks):\n",
      "        \"\"\"Modify video to consist of chunks.  To append video B to the end of video A:\n",
      "           a.append(a.chunks() + b.chunks())\"\"\"\n",
      "        \n",
      "        if not self.writable:\n",
      "            raise Exception('Please instantiate MP4 with writable=True in order to call update_in_place_with_chunks')\n",
      "        \n",
      "        # Make sure video has moov, free, mdat in that order\n",
      "        if not 'free' in self.info:\n",
      "            raise NeedsRewriteException('missing free section')\n",
      "        \n",
      "        if (self.info['moov']['_position'] > self.info['free']['_position'] or\n",
      "            self.info['free']['_position'] > self.info['mdat']['_position']):\n",
      "            raise NeedsRewriteException('sections disordered')\n",
      "        \n",
      "        moov = copy.deepcopy(self.info['moov'])\n",
      "\n",
      "        # Compute duration\n",
      "        sample_duration = moov['trak']['mdia']['minf']['stbl']['stts']['time_to_sample_map'][0]['sample_duration']\n",
      "        mdhd_time_scale = moov['trak']['mdia']['mdhd']['time_scale']\n",
      "        nsamples = 0\n",
      "        for chunk in chunks:\n",
      "            nsamples += len(chunk.sample_sizes)\n",
      "\n",
      "        fps = float(mdhd_time_scale) / sample_duration\n",
      "        duration = float(nsamples * sample_duration) / mdhd_time_scale\n",
      "\n",
      "        print 'New duration %.3f sec (%d samples at %.5f FPS)' % (duration, nsamples, fps)\n",
      "\n",
      "        # Adjust mdhd (media header) duration\n",
      "        moov['trak']['mdia']['mdhd']['duration'] = sample_duration * nsamples\n",
      "\n",
      "        # Adjust mvhd (movie header) duration\n",
      "        moov['mvhd']['duration'] = int(duration * moov['mvhd']['time_scale'] + 0.5)\n",
      "\n",
      "        # Adjust tkhd (track header) duration\n",
      "        moov['trak']['tkhd']['duration'] = int(duration * moov['mvhd']['time_scale'] + 0.5)\n",
      "\n",
      "        # Adjust elst (edit list) duration\n",
      "        edits = moov['trak']['edts']['elst']['edits']\n",
      "        assert len(edits) == 1 # If more than one edit, we need new code to handle\n",
      "        assert edits[0]['rate'] == 65536 # 1.0 in fixed point\n",
      "        edits[0]['duration'] = int(duration * moov['mvhd']['time_scale'] + 0.5)\n",
      "\n",
      "        # Construct new list of offsets for stco (chunk offsets)\n",
      "        stco = moov['trak']['mdia']['minf']['stbl']['stco']\n",
      "        stco['chunk_offsets'] = []\n",
      "        pos = self.info['mdat']['_position'] + 8  # Position of first chunk\n",
      "        for chunk in chunks:\n",
      "            stco['chunk_offsets'].append(pos)\n",
      "            pos += chunk.length\n",
      "\n",
      "        # Construct new list of all sample (frame) sizes for stsz (sample sizes)\n",
      "        stsz = moov['trak']['mdia']['minf']['stbl']['stsz']\n",
      "        stsz['sample_sizes'] = []\n",
      "        for chunk in chunks:\n",
      "           stsz['sample_sizes'].extend(chunk.sample_sizes)\n",
      "\n",
      "        # Construct new list of entries for stsc (chunk to sample map)\n",
      "        stsc = moov['trak']['mdia']['minf']['stbl']['stsc']\n",
      "        stsc['sample_to_chunk_map'] = []\n",
      "        for (i, chunk) in enumerate(chunks):\n",
      "            stsc['sample_to_chunk_map'].append({\n",
      "                'first_chunk': i + 1,\n",
      "                'samples_per_chunk': len(chunk.sample_sizes),\n",
      "                'sample_description_id': 1\n",
      "            })\n",
      "\n",
      "        # Construct new list of keyframes for stss (keyframe list)\n",
      "        stss = moov['trak']['mdia']['minf']['stbl']['stss']\n",
      "        stss['key_frame_samples'] = []\n",
      "        sampleno = 1 # 1-based\n",
      "        for chunk in chunks:\n",
      "            for keyframe in chunk.keyframes:\n",
      "                stss['key_frame_samples'].append(sampleno + keyframe)\n",
      "            sampleno += len(chunk.sample_sizes)\n",
      "\n",
      "        # Adjust sample count for stts (time to sample map)\n",
      "        stts = moov['trak']['mdia']['minf']['stbl']['stts']\n",
      "        assert len(stts['time_to_sample_map']) == 1\n",
      "        stts['time_to_sample_map'][0]['sample_count'] = nsamples\n",
      "        \n",
      "        # Create new moov section\n",
      "        moov_out = self.write_atom(moov)\n",
      "        free_len = self.info['mdat']['_position'] - moov['_position'] - len(moov_out) - 8\n",
      "        if free_len < 0:\n",
      "            raise NeedsRewriteException('not enough free space', -free_len)\n",
      "\n",
      "        # Modify mdat in place with new chunks\n",
      "        self.fp.seek(self.info['mdat']['_position'])\n",
      "\n",
      "        # Compute new length of mdat in bytes\n",
      "        length = 8\n",
      "        for chunk in chunks:\n",
      "            length += chunk.length\n",
      "\n",
      "        begin_mdat = self.fp.tell()\n",
      "        self.fp.write(self.write32(length))\n",
      "        self.fp.write('mdat')\n",
      "\n",
      "        for chunk in chunks:\n",
      "            # Copying from self?  Make sure we haven't tried to move the chunk\n",
      "            if chunk.video == self:\n",
      "                assert chunk.offset == self.fp.tell()\n",
      "                # Skip\n",
      "                self.fp.seek(chunk.length + self.fp.tell())\n",
      "            else:\n",
      "                chunk.video.fp.seek(chunk.offset)\n",
      "                self.fp.write(chunk.video.fp.read(chunk.length))\n",
      "\n",
      "        assert(self.fp.tell() == begin_mdat + length)\n",
      "\n",
      "        self.fp.truncate()\n",
      "\n",
      "        # Rewrite moov and free\n",
      "\n",
      "        self.fp.seek(moov['_position'])\n",
      "        print 'Writing moov (%d bytes) at position %d' % (len(moov_out), self.fp.tell())\n",
      "        self.fp.write(moov_out)\n",
      "        \n",
      "        print 'Writing free (%d bytes) at position %d (end=%d)' % (free_len + 8, self.fp.tell(), \n",
      "                                                                   self.fp.tell() + free_len + 8)\n",
      "        self.fp.write(self.write_free_atom(free_len))\n",
      "        assert self.fp.tell() == self.info['mdat']['_position']\n",
      "\n",
      "        print 'Updated %s, length %d' % (self.filename, os.stat(self.filename).st_size)    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def parse_filename_and_chunks(filename, writable=False):\n",
      "    match = re.match(r'(.*)(\\[(-?\\d+)?\\:(-?\\d+)?\\])', filename)\n",
      "    if match:\n",
      "        filename = match.groups()[0]\n",
      "        groups = match.groups()[1]\n",
      "    else:\n",
      "        groups = ''\n",
      "\n",
      "    chunks = MP4(filename, writable=writable).chunks()\n",
      "    chunks = eval('chunks' + groups)\n",
      "    return chunks\n",
      "\n",
      "print parse_filename_and_chunks('short.mp4[0:-1]')\n",
      "print parse_filename_and_chunks('short.mp4')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Read short.mp4\n",
        "[Chunk(video=short.mp4, index=0, offset=1389, nsamples=60, length=1017531)]\n",
        "Read short.mp4\n",
        "[Chunk(video=short.mp4, index=0, offset=1389, nsamples=60, length=1017531), Chunk(video=short.mp4, index=1, offset=1018920, nsamples=67, length=1021295)]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def append(filenames_and_chunks, future_frames=1000):\n",
      "    while True:\n",
      "        chunks = parse_filename_and_chunks(filenames_and_chunks[0], writable=True)\n",
      "\n",
      "        for file in filenames_and_chunks[1:]:\n",
      "            chunks.extend(parse_filename_and_chunks(file))\n",
      "\n",
      "        dest = chunks[0].video\n",
      "\n",
      "        try:\n",
      "            dest.update_in_place_using_chunks(chunks)\n",
      "        except NeedsRewriteException as e:\n",
      "            print e\n",
      "            # Assume approx 6 bytes per frame\n",
      "            padding = max(future_frames * 6, dest.info['moov']['atomsize'])\n",
      "            free = e.space_needed + padding\n",
      "            print 'rewriting video with free=%d' % free\n",
      "            tmpname = '%s-tmp%d' % (dest.filename, os.getpid())\n",
      "            dest.copy_with_padding(tmpname, free)\n",
      "            os.rename(tmpname, dest.filename)\n",
      "            continue\n",
      "        break\n",
      "\n",
      "import shutil\n",
      "\n",
      "shutil.copyfile('short.mp4', 'test1.mp4')\n",
      "shutil.copyfile('short.mp4', 'test2.mp4')\n",
      "append(['test1.mp4', 'test2.mp4'], future_frames=2000)\n",
      "\n",
      "shutil.copyfile('short.mp4', 'test3.mp4')\n",
      "shutil.copyfile('short.mp4', 'test4.mp4')\n",
      "shutil.copyfile('short.mp4', 'test5.mp4')\n",
      "append(['test3.mp4[0:-1]', 'test4.mp4', 'test5.mp4[1:]'], future_frames=2000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Read test1.mp4\n",
        "Read test2.mp4\n",
        "New duration 21.167 sec (254 samples at 12.00000 FPS)\n",
        "'Video needs rewriting because not enough free space (space needed=592)'\n",
        "rewriting video with free=12592\n",
        "Moving mdat by 12592 bytes\n",
        "Wrote test1.mp4-tmp3659\n",
        "Read test1.mp4\n",
        "Read test2.mp4\n",
        "New duration 21.167 sec (254 samples at 12.00000 FPS)\n",
        "Writing moov (1933 bytes) at position 32\n",
        "Writing free (12008 bytes) at position 1965 (end=13973)\n",
        "Updated test1.mp4, length 4091633\n",
        "Read test3.mp4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Read test4.mp4\n",
        "Read test5.mp4\n",
        "New duration 21.167 sec (254 samples at 12.00000 FPS)\n",
        "'Video needs rewriting because not enough free space (space needed=592)'\n",
        "rewriting video with free=12592\n",
        "Moving mdat by 12592 bytes\n",
        "Wrote test3.mp4-tmp3659\n",
        "Read test3.mp4\n",
        "Read test4.mp4\n",
        "Read test5.mp4\n",
        "New duration 21.167 sec (254 samples at 12.00000 FPS)\n",
        "Writing moov (1933 bytes) at position 32\n",
        "Writing free (12008 bytes) at position 1965 (end=13973)\n",
        "Updated test3.mp4, length 4091633\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!open test3.mp4"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# How to concatenate two videos using ffmpeg\n",
      "\n",
      "#!ffmpeg -i short.mp4 -c copy -bsf:v h264_mp4toannexb -f mpegts -y short.ts\n",
      "#!ffmpeg -i \"concat:short.ts|short.ts\" -c copy -y combined.mp4\n",
      "#!ls -l short.mp4 combined.mp4"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ffmpeg version 2.5.1 Copyright (c) 2000-2014 the FFmpeg developers\r\n",
        "  built on Jan 28 2015 06:43:59 with Apple LLVM version 6.0 (clang-600.0.56) (based on LLVM 3.5svn)\r\n",
        "  configuration: --prefix=/opt/local --enable-swscale --enable-avfilter --enable-avresample --enable-libmp3lame --enable-libvorbis --enable-libopus --enable-libtheora --enable-libschroedinger --enable-libopenjpeg --enable-libmodplug --enable-libvpx --enable-libspeex --enable-libass --enable-libbluray --enable-lzma --enable-gnutls --enable-fontconfig --enable-libfreetype --enable-libfribidi --disable-libxcb --disable-libxcb-shm --disable-libxcb-xfixes --disable-indev=jack --disable-outdev=xv --mandir=/opt/local/share/man --enable-shared --enable-pthreads --cc=/usr/bin/clang --enable-vda --arch=x86_64 --enable-yasm --enable-gpl --enable-postproc --enable-libx264 --enable-libxvid\r\n",
        "  libavutil      54. 15.100 / 54. 15.100\r\n",
        "  libavcodec     56. 13.100 / 56. 13.100\r\n",
        "  libavformat    56. 15.102 / 56. 15.102\r\n",
        "  libavdevice    56.  3.100 / 56.  3.100\r\n",
        "  libavfilter     5.  2.103 /  5.  2.103\r\n",
        "  libavresample   2.  1.  0 /  2.  1.  0\r\n",
        "  libswscale      3.  1.101 /  3.  1.101\r\n",
        "  libswresample   1.  1.100 /  1.  1.100\r\n",
        "  libpostproc    53.  3.100 / 53.  3.100\r\n",
        "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'short.mp4':\r\n",
        "  Metadata:\r\n",
        "    major_brand     : isom\r\n",
        "    minor_version   : 512\r\n",
        "    compatible_brands: isomiso2avc1mp41\r\n",
        "    encoder         : Lavf56.15.102\r\n",
        "  Duration: 00:00:10.58, start: 0.000000, bitrate: 1542 kb/s\r\n",
        "    Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 1424x800, 1541 kb/s, 12 fps, 12 tbr, 12288 tbn, 24 tbc (default)\r\n",
        "    Metadata:\r\n",
        "      handler_name    : VideoHandler\r\n",
        "Output #0, mpegts, to 'short.ts':\r\n",
        "  Metadata:\r\n",
        "    major_brand     : isom\r\n",
        "    minor_version   : 512\r\n",
        "    compatible_brands: isomiso2avc1mp41\r\n",
        "    encoder         : Lavf56.15.102\r\n",
        "    Stream #0:0(und): Video: h264 (avc1 / 0x31637661), yuv420p, 1424x800, q=2-31, 1541 kb/s, 12 fps, 90k tbn, 12 tbc (default)\r\n",
        "    Metadata:\r\n",
        "      handler_name    : VideoHandler\r\n",
        "Stream mapping:\r\n",
        "  Stream #0:0 -> #0:0 (copy)\r\n",
        "Press [q] to stop, [?] for help\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "frame=  127 fps=0.0 q=-1.0 Lsize=    2165kB time=00:00:10.58 bitrate=1676.2kbits/s    \r\n",
        "video:1992kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 8.736012%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ffmpeg version 2.5.1 Copyright (c) 2000-2014 the FFmpeg developers\r\n",
        "  built on Jan 28 2015 06:43:59 with Apple LLVM version 6.0 (clang-600.0.56) (based on LLVM 3.5svn)\r\n",
        "  configuration: --prefix=/opt/local --enable-swscale --enable-avfilter --enable-avresample --enable-libmp3lame --enable-libvorbis --enable-libopus --enable-libtheora --enable-libschroedinger --enable-libopenjpeg --enable-libmodplug --enable-libvpx --enable-libspeex --enable-libass --enable-libbluray --enable-lzma --enable-gnutls --enable-fontconfig --enable-libfreetype --enable-libfribidi --disable-libxcb --disable-libxcb-shm --disable-libxcb-xfixes --disable-indev=jack --disable-outdev=xv --mandir=/opt/local/share/man --enable-shared --enable-pthreads --cc=/usr/bin/clang --enable-vda --arch=x86_64 --enable-yasm --enable-gpl --enable-postproc --enable-libx264 --enable-libxvid\r\n",
        "  libavutil      54. 15.100 / 54. 15.100\r\n",
        "  libavcodec     56. 13.100 / 56. 13.100\r\n",
        "  libavformat    56. 15.102 / 56. 15.102\r\n",
        "  libavdevice    56.  3.100 / 56.  3.100\r\n",
        "  libavfilter     5.  2.103 /  5.  2.103\r\n",
        "  libavresample   2.  1.  0 /  2.  1.  0\r\n",
        "  libswscale      3.  1.101 /  3.  1.101\r\n",
        "  libswresample   1.  1.100 /  1.  1.100\r\n",
        "  libpostproc    53.  3.100 / 53.  3.100\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Input #0, mpegts, from 'concat:short.ts|short.ts':\r\n",
        "  Duration: 00:00:10.58, start: 1.400000, bitrate: 3352 kb/s\r\n",
        "  Program 1 \r\n",
        "    Metadata:\r\n",
        "      service_name    : Service01\r\n",
        "      service_provider: FFmpeg\r\n",
        "    Stream #0:0[0x100]: Video: h264 (High) ([27][0][0][0] / 0x001B), yuv420p, 1424x800, 12 fps, 12 tbr, 90k tbn, 24 tbc\r\n",
        "Output #0, mp4, to 'combined.mp4':\r\n",
        "  Metadata:\r\n",
        "    encoder         : Lavf56.15.102\r\n",
        "    Stream #0:0: Video: h264 ([33][0][0][0] / 0x0021), yuv420p, 1424x800, q=2-31, 12 fps, 90k tbn, 90k tbc\r\n",
        "Stream mapping:\r\n",
        "  Stream #0:0 -> #0:0 (copy)\r\n",
        "Press [q] to stop, [?] for help\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u001b[0;35m[mpegts @ 0x7fd8c180da00] \u001b[0m\u001b[0;33mDTS 126000 < 1071000 out of order\r\n",
        "\u001b[0mframe=  254 fps=0.0 q=-1.0 Lsize=    3986kB time=00:00:21.08 bitrate=1548.9kbits/s    \r\n",
        "video:3985kB audio:0kB subtitle:0kB other streams:0kB global headers:1kB muxing overhead: 0.048601%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-rw-r--r--  1 rsargent  staff  4082119 Jan 28 06:50 combined.mp4\r\n",
        "-rw-rw-r--  1 rsargent  staff  2040215 Jan 24 16:00 short.mp4\r\n"
       ]
      }
     ],
     "prompt_number": 45
    }
   ],
   "metadata": {}
  }
 ]
}